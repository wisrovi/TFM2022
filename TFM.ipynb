{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El dataset se puede descargar de: https://www.kaggle.com/datasets/imsparsh/musicnet-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez descargado se debe poner en la carpeta /kaggle/input/musicnet-dataset todo el contenido de la descarga"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "El dataset tiene dos formatos:\n",
    "\n",
    ".wav: archivo de audio estándar ~ grabación\n",
    "\n",
    ".mid: formato midi (interfaz digital de instrumentos musicales) ( https://en.wikipedia.org/wiki/MIDI )\n",
    "\n",
    "Es un estándar técnico que describe un protocolo de comunicaciones que conecta instrumentos musicales electrónicos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la ejecucipn de este cuaderno se deben instalar las siguientes librerias (en las versiones establecidas), tal como se muestra a continuacion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install pandas\n",
    "!pip install librosa==0.8.1\n",
    "!pip install numpy==1.21\n",
    "!pip install python_speech_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Preprocesar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from libraries.Kaggle_audios import Kaggle_audios\n",
    "from libraries.ProcessAudio import ProcessAudio\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIEMPO_SELECCIONADO = 3\n",
    "ARCHIVO_FINAL_TRAIN = \"data/scaler_pca_to_use/train\"\n",
    "ARCHIVO_FINAL_TEST = \"data/scaler_pca_to_use/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "\n",
    "def count_elapsed_time(f):\n",
    "    @wraps(f)\n",
    "    def cronometro(*args, **kwargs):\n",
    "        t_inicial = time()  # tomo la hora antes de ejecutar la funcion\n",
    "        salida = f(*args, **kwargs)\n",
    "        t_final = time()  # tomo la hora despues de ejecutar la funcion\n",
    "        print('Tiempo transcurrido (en segundos): {}'.format(t_final - t_inicial))\n",
    "        return salida\n",
    "\n",
    "    return cronometro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_tensorflow(data):\n",
    "    train = data[:, 1:-11]\n",
    "    instrument_list = data[:,-11:]\n",
    "    X = np.array(train, dtype=float)\n",
    "    y = instrument_list\n",
    "    return X, y\n",
    "\n",
    "def aplicando_pca(scaler_pca, name_file, dataX, dataY):\n",
    "    x_for_model = scaler_pca.transform(X=dataX)\n",
    "    # print(\"Original\", X.shape)\n",
    "    np.savez_compressed(name_file + \".npz\", data=x_for_model)\n",
    "    np.savez_compressed(name_file + \"_label.npz\", data=dataY)\n",
    "    \n",
    "def leyendo_datos(name_file):\n",
    "    x = np.load(name_file + \".npz\")['data']\n",
    "    y = np.load(name_file + \"_label.npz\")['data']\n",
    "    return x, y\n",
    "\n",
    "def Preprocesar_audios(name_file, use_train, save:bool = True):\n",
    "    cortar = Kaggle_audios(config_time=TIEMPO_SELECCIONADO, train=use_train)\n",
    "\n",
    "    print(\"\\tLeyendo todos los archivos WAV originales\")\n",
    "    all_data, all_label, rate = cortar.read_data(\n",
    "        limit=None, show_info=False)  # leer todos los wav y cada uno separarlos en pequeños audios de 3 segundos\n",
    "\n",
    "    print(\"\\tTime:\", TIEMPO_SELECCIONADO, \" - Input:\", all_data.shape, \" - Output:\", all_label.shape, \" - rate:\", rate)\n",
    "\n",
    "    print(\"\\tExtrayendo caracteristicas audios\")\n",
    "    data = list()\n",
    "    for id_audio, x in enumerate(all_data):\n",
    "        processAudio = ProcessAudio()\n",
    "        processAudio.set_data(x)\n",
    "        data_save = processAudio.get_all(id_audio)  # Extrayendo caracteristicas audios, salen 26 caracteristicas\n",
    "        data_save += all_label[id_audio].tolist()\n",
    "\n",
    "        data.append(data_save)\n",
    "        try:\n",
    "            if id_audio%150 == 0:\n",
    "                print(\"\\n\\t\", end=\"\")\n",
    "            print(\".\", end=\"\")\n",
    "        except:\n",
    "            print()\n",
    "\n",
    "    print(\"\\tConvirtiendo a numpy\")\n",
    "    data = np.array(data)\n",
    "\n",
    "    if save:\n",
    "        print(\"\\tGuardando data\")\n",
    "        np.savez_compressed(name_file + '.npz', data)\n",
    "        print(\"\\tGuardando csv completo\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Leer audios, separarlos en audios mas pequeños y extraer las caracteristicas de cada uno de estos nuevos audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesando TRAIN\n",
      "/kaggle/input/musicnet-dataset/musicnet/musicnet/train_data/\n",
      "/kaggle/input/musicnet-dataset/musicnet/musicnet/train_labels/\n",
      "\tLeyendo todos los archivos WAV originales\n",
      "\n",
      "\t. . . . "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocesando TRAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data_train \u001b[38;5;241m=\u001b[39m \u001b[43mPreprocesar_audios\u001b[49m\u001b[43m(\u001b[49m\u001b[43mARCHIVO_FINAL_TRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;124m\"\u001b[39m, TIEMPO_SELECCIONADO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Train:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data_train))\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mPreprocesar_audios\u001b[0;34m(name_file, use_train, save)\u001b[0m\n\u001b[1;32m     20\u001b[0m cortar \u001b[38;5;241m=\u001b[39m Kaggle_audios(config_time\u001b[38;5;241m=\u001b[39mTIEMPO_SELECCIONADO, train\u001b[38;5;241m=\u001b[39muse_train)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLeyendo todos los archivos WAV originales\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m all_data, all_label, rate \u001b[38;5;241m=\u001b[39m \u001b[43mcortar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# leer todos los wav y cada uno separarlos en pequeños audios de 3 segundos\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mTime:\u001b[39m\u001b[38;5;124m\"\u001b[39m, TIEMPO_SELECCIONADO, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Input:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_data\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_label\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - rate:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rate)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mExtrayendo caracteristicas audios\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/tf/libraries/Kaggle_audios.py:79\u001b[0m, in \u001b[0;36mKaggle_audios.read_data\u001b[0;34m(self, limit, show_info)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mReading: id:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpartial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - file:\u001b[39m\u001b[38;5;124m\"\u001b[39m, resource_read, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - len before:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_data))\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# print(resource_read + \".wav\", len(muestras_wav), type(muestras_wav))\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# print(resource_read + \".csv\", len(instrumentos), type(instrumentos))\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m all_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__correccion_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmuestras_wav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m all_label \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__correcion_instrumentos(instrumentos)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tf/libraries/Kaggle_audios.py:52\u001b[0m, in \u001b[0;36mKaggle_audios.__correccion_data\u001b[0;34m(self, muestras_wav)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__correccion_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, muestras_wav):\n\u001b[1;32m     51\u001b[0m     c \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(muestras_wav[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(muestras_wav[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))]\n\u001b[0;32m---> 52\u001b[0m     muestras_wav[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmuestras_wav\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m muestras_wav\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Preprocesando TRAIN\")\n",
    "data_train = Preprocesar_audios(ARCHIVO_FINAL_TRAIN, use_train=True, save=False)\n",
    "print(\"\\tTime:\", TIEMPO_SELECCIONADO, \" - Train:\", len(data_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Hallando el normalizador y el PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _ = prepare_data_tensorflow(data_train)\n",
    "scaler_pca = make_pipeline(StandardScaler(), PCA())\n",
    "scaler_pca.fit(X_train)\n",
    "\n",
    "modelo_pca = scaler_pca.named_steps['pca']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelo_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Porcentaje de varianza explicada acumulada\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m prop_varianza_acum \u001b[38;5;241m=\u001b[39m \u001b[43mmodelo_pca\u001b[49m\u001b[38;5;241m.\u001b[39mexplained_variance_ratio_\u001b[38;5;241m.\u001b[39mcumsum()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m------------------------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPorcentaje de varianza explicada acumulada\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelo_pca' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Porcentaje de varianza explicada acumulada\n",
    "# ==============================================================================\n",
    "prop_varianza_acum = modelo_pca.explained_variance_ratio_.cumsum()\n",
    "print('------------------------------------------')\n",
    "print('Porcentaje de varianza explicada acumulada')\n",
    "print('------------------------------------------')\n",
    "print(prop_varianza_acum)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 20))\n",
    "ax.plot(\n",
    "    np.arange(26) + 1,\n",
    "    prop_varianza_acum,\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(26) + 1, prop_varianza_acum):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x, y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 10),\n",
    "        ha='center',\n",
    "        fontsize=20\n",
    "    )\n",
    "\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_xticks(np.arange(modelo_pca.n_components_) + 1)\n",
    "ax.set_title('Porcentaje de varianza explicada acumulada')\n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Por. varianza acumulada')\n",
    "\n",
    "ax.axvline(x=15, color=\"red\")\n",
    "ax.axhline(y=0.925, color=\"red\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "se elige el 92% como valor de varianza explicada, pues con esta varianza de 8% menos del total se logra una reduccion de entradas del 42% (26 a 15) con lo que es un buen negocio que le facilita al modelo el aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('pca', PCA(n_components=0.92))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "            Hallando el Normalizador y el PCA (92%)\n",
    "\"\"\"\n",
    "\n",
    "MINIMA_VARIANA_EXPLICADA = 0.92\n",
    "scaler_pca = make_pipeline(StandardScaler(), PCA(MINIMA_VARIANA_EXPLICADA))\n",
    "scaler_pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler_pca, open('data/scaler_pca_to_use/scaler_pca.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Aplicando el normalizador, PCA a los datos train y guardando el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "            Aplicando PCA\n",
    "\"\"\"\n",
    "X_train, Y_train = prepare_data_tensorflow(data_train)\n",
    "aplicando_pca(scaler_pca, ARCHIVO_FINAL_TRAIN, X_train, Y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Aplicando el normalizador, PCA a los datos test y guardando el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesando TEST\n",
      "/kaggle/input/musicnet-dataset/musicnet/musicnet/test_data/\n",
      "/kaggle/input/musicnet-dataset/musicnet/musicnet/test_labels/\n",
      "\tLeyendo todos los archivos WAV originales\n",
      "\n",
      "\t. . . . . . . . . . \tTime: 3  - Input: (499, 132300)  - Output: (499, 11)  - rate: 44100\n",
      "\tExtrayendo caracteristicas audios\n",
      "\n",
      "\t............................................................................................................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/librosa/core/pitch.py:153: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn(\"Trying to estimate tuning from empty frequency set.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\n",
      "\t......................................................................................................................................................\n",
      "\t......................................................................................................................................................\n",
      "\t.................................................\tConvirtiendo a numpy\n",
      "\tTime: 3  - Test: 499\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocesando TEST\")\n",
    "data_test = Preprocesar_audios(ARCHIVO_FINAL_TEST, False, save=False)\n",
    "print(\"\\tTime:\", TIEMPO_SELECCIONADO, \" - Test:\", len(data_test))\n",
    "\n",
    "X_test, Y_test = prepare_data_tensorflow(data_test)\n",
    "aplicando_pca(scaler_pca, ARCHIVO_FINAL_TEST, X_test, Y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Cargando los datos para usarlos en el entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_for_model_charge_train: (40625, 15) (40625, 11)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "            Leyendo datos\n",
    "\"\"\"\n",
    "xTrain_for_model_charge, yTrain = leyendo_datos(ARCHIVO_FINAL_TRAIN)\n",
    "print(\"x_for_model_charge_train:\", xTrain_for_model_charge.shape, yTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_for_model_charge_test: (499, 15) (499, 11)\n"
     ]
    }
   ],
   "source": [
    "xTest_for_model_charge, yTest = leyendo_datos(ARCHIVO_FINAL_TEST)\n",
    "print(\"x_for_model_charge_test:\", xTest_for_model_charge.shape, yTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Creando y entrenando modelos modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Modelo clasico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = lambda lst: int((sum(lst) / len(lst))*100)/100\n",
    "\n",
    "\n",
    "def calcular_porcentajes_aciertos(y_f, y_t):\n",
    "    verdaderos = dict()\n",
    "    falsos = dict()\n",
    "    for j in range(y_f.shape[1]):\n",
    "        verdaderos[j] = 0\n",
    "        falsos[j] = 0\n",
    "\n",
    "    for i in range(y_f.shape[0]):\n",
    "        for j in range(y_f.shape[1]):\n",
    "            if y_f[i][j] == y_t[i][j]:\n",
    "                verdaderos[j] += 1\n",
    "            else:\n",
    "                falsos[j] += 1\n",
    "\n",
    "    for j in range(y_f.shape[1]):\n",
    "        #y_final.shape[1] -> 100%\n",
    "        #verdaderos[j]    -> X\n",
    "        verdaderos[j] = int(verdaderos[j] * 100 / y_f.shape[0])\n",
    "        falsos[j] = int(falsos[j] * 100 / y_f.shape[0])\n",
    "\n",
    "    return verdaderos, falsos, str(mean([v for i, v in verdaderos.items()])) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1)  Dividiendo datos para train y valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xTrain_for_model_charge, yTrain, test_size=0.1)  # 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Entrenando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForestClassifier] Aciertos train 100.0%\n",
      "[RandomForestClassifier] Aciertos validacion 95.9%\n",
      "[RandomForestClassifier] Aciertos test 94.81%\n"
     ]
    }
   ],
   "source": [
    "algoritmo_clasico = RandomForestClassifier()\n",
    "algoritmo_clasico = MultiOutputClassifier(algoritmo_clasico, n_jobs=-1)\n",
    "\n",
    "seed = 1\n",
    "grid = GridSearchCV(\n",
    "          estimator = algoritmo_clasico,\n",
    "          param_grid={},\n",
    "          cv = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        )\n",
    "grid.fit(X_train, y_train)\n",
    "algoritmo_clasico = grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"[RandomForestClassifier] Aciertos train\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"[RandomForestClassifier] Aciertos validacion\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"[RandomForestClassifier] Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiOutputClassifier' object has no attribute 'tree_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_graphviz\n\u001b[0;32m----> 2\u001b[0m \u001b[43mexport_graphviz\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgoritmo_clasico\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrounded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_export.py:889\u001b[0m, in \u001b[0;36mexport_graphviz\u001b[0;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision, fontname)\u001b[0m\n\u001b[1;32m    870\u001b[0m     out_file \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[1;32m    872\u001b[0m exporter \u001b[38;5;241m=\u001b[39m _DOTTreeExporter(\n\u001b[1;32m    873\u001b[0m     out_file\u001b[38;5;241m=\u001b[39mout_file,\n\u001b[1;32m    874\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     fontname\u001b[38;5;241m=\u001b[39mfontname,\n\u001b[1;32m    888\u001b[0m )\n\u001b[0;32m--> 889\u001b[0m \u001b[43mexporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_string:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exporter\u001b[38;5;241m.\u001b[39mout_file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_export.py:462\u001b[0m, in \u001b[0;36m_DOTTreeExporter.export\u001b[0;34m(self, decision_tree)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse(decision_tree, \u001b[38;5;241m0\u001b[39m, criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpurity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse(\u001b[43mdecision_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m, \u001b[38;5;241m0\u001b[39m, criterion\u001b[38;5;241m=\u001b[39mdecision_tree\u001b[38;5;241m.\u001b[39mcriterion)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtail()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiOutputClassifier' object has no attribute 'tree_'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(algoritmo_clasico,\n",
    "                filled=True,\n",
    "                rounded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegression] Aciertos train 92.63%\n",
      "[LogisticRegression] Aciertos validacion 92.36%\n",
      "[LogisticRegression] Aciertos test 91.18%\n"
     ]
    }
   ],
   "source": [
    "algoritmo_clasico = LogisticRegression(\n",
    "          solver='sag', #'liblinear', 'sag'\n",
    "          max_iter=1000, \n",
    "          random_state=1, #0 ,1\n",
    "          multi_class='ovr'\n",
    "          )\n",
    "algoritmo_clasico = MultiOutputClassifier(algoritmo_clasico, n_jobs=-1)\n",
    "\n",
    "seed = 1\n",
    "grid = GridSearchCV(\n",
    "          estimator = algoritmo_clasico,\n",
    "          param_grid={},\n",
    "          cv = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        )\n",
    "grid.fit(X_train, y_train)\n",
    "algoritmo_clasico = grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"[LogisticRegression] Aciertos train\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"[LogisticRegression] Aciertos validacion\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"[LogisticRegression] Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DecisionTreeClassifier] Aciertos train 100.0%\n",
      "[DecisionTreeClassifier] Aciertos validacion 92.9%\n",
      "[DecisionTreeClassifier] Aciertos test 92.9%\n"
     ]
    }
   ],
   "source": [
    "algoritmo_clasico = DecisionTreeClassifier(\n",
    "          max_depth = None,\n",
    "          min_samples_split = 2,\n",
    "          min_samples_leaf  = 1,\n",
    "          random_state      = 123\n",
    "          )\n",
    "algoritmo_clasico = MultiOutputClassifier(algoritmo_clasico, n_jobs=-1)\n",
    "\n",
    "seed = 1\n",
    "grid = GridSearchCV(\n",
    "          estimator = algoritmo_clasico,\n",
    "          param_grid={},\n",
    "          cv = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        )\n",
    "grid.fit(X_train, y_train)\n",
    "algoritmo_clasico = grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"[DecisionTreeClassifier] Aciertos train\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"[DecisionTreeClassifier] Aciertos validacion\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"[DecisionTreeClassifier] Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: `n_features_in_` is deprecated in 1.0 and will be removed in 1.2.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DummyClassifier] Aciertos train 85.18%\n",
      "[DummyClassifier] Aciertos validacion 85.27%\n",
      "[DummyClassifier] Aciertos test 80.9%\n"
     ]
    }
   ],
   "source": [
    "algoritmo_clasico = DummyClassifier()\n",
    "algoritmo_clasico = MultiOutputClassifier(algoritmo_clasico, n_jobs=-1)\n",
    "\n",
    "seed = 1\n",
    "grid = GridSearchCV(\n",
    "          estimator = algoritmo_clasico,\n",
    "          param_grid={},\n",
    "          cv = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        )\n",
    "grid.fit(X_train, y_train)\n",
    "algoritmo_clasico = grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"[DummyClassifier] Aciertos train\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"[DummyClassifier] Aciertos validacion\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"[DummyClassifier] Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNeighborsClassifier] Aciertos train 97.09%\n",
      "[KNeighborsClassifier] Aciertos validacion 96.36%\n",
      "[KNeighborsClassifier] Aciertos test 96.09%\n"
     ]
    }
   ],
   "source": [
    "algoritmo_clasico = KNeighborsClassifier(11)\n",
    "algoritmo_clasico = MultiOutputClassifier(algoritmo_clasico, n_jobs=-1)\n",
    "\n",
    "seed = 1\n",
    "grid = GridSearchCV(\n",
    "          estimator = algoritmo_clasico,\n",
    "          param_grid={},\n",
    "          cv = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        )\n",
    "grid.fit(X_train, y_train)\n",
    "algoritmo_clasico = grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"[KNeighborsClassifier] Aciertos train\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"[KNeighborsClassifier] Aciertos validacion\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"[KNeighborsClassifier] Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GaussianNB] Aciertos train 90.72%\n",
      "[GaussianNB] Aciertos validacion 90.54%\n",
      "[GaussianNB] Aciertos test 89.36%\n"
     ]
    }
   ],
   "source": [
    "algoritmo_clasico = GaussianNB()\n",
    "algoritmo_clasico = MultiOutputClassifier(algoritmo_clasico, n_jobs=-1)\n",
    "\n",
    "seed = 1\n",
    "grid = GridSearchCV(\n",
    "          estimator = algoritmo_clasico,\n",
    "          param_grid={},\n",
    "          cv = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        )\n",
    "grid.fit(X_train, y_train)\n",
    "algoritmo_clasico = grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"[GaussianNB] Aciertos train\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"[GaussianNB] Aciertos validacion\", aciertos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_final = algoritmo_clasico.predict(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"[GaussianNB] Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Guardando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(algoritmo_clasico, open('data/scaler_pca_to_use/KNeighborsClassifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) Cargando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = pickle.load(open('data/scaler_pca_to_use/KNeighborsClassifier.pkl', 'rb'))\n",
    "model = pickle.load(open('data/scaler_pca_to_use/randomforest.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5) Probando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aciertos train 94.9%\n"
     ]
    }
   ],
   "source": [
    "y_final = model.predict(X_train)\n",
    "\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"Aciertos train\", aciertos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Validando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aciertos validacion 94.9%\n"
     ]
    }
   ],
   "source": [
    "y_final = model.predict(X_test)\n",
    "\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"Aciertos validacion\", aciertos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4) Testeando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo transcurrido (en segundos): 0.8659086227416992\n",
      "Aciertos test 93.18%\n"
     ]
    }
   ],
   "source": [
    "@count_elapsed_time\n",
    "def predecir():\n",
    "    y_final = model.predict(xTest_for_model_charge)\n",
    "    return y_final\n",
    "\n",
    "y_final = predecir()\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Creando y entrenando modelo red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datos train: (40625, 15) (40625, 11)\n",
      "datos test: (499, 15) (499, 11)\n"
     ]
    }
   ],
   "source": [
    "xTrain_for_model_charge, yTrain = leyendo_datos(ARCHIVO_FINAL_TRAIN)\n",
    "yTrain = np.array(yTrain, dtype=float)\n",
    "xTrain_for_model_charge = np.array(xTrain_for_model_charge, dtype=float)\n",
    "print(\"datos train:\", xTrain_for_model_charge.shape, yTrain.shape)\n",
    "\n",
    "xTest_for_model_charge, yTest = leyendo_datos(ARCHIVO_FINAL_TEST)\n",
    "yTest = np.array(yTest, dtype=float)\n",
    "xTest_for_model_charge = np.array(xTest_for_model_charge, dtype=float)\n",
    "print(\"datos test:\", xTest_for_model_charge.shape, yTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, LeakyReLU, Conv1D, Conv2D, Flatten, MaxPooling2D, Input\n",
    "from tensorflow.keras.layers import BatchNormalization, InputLayer, Reshape, Activation, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import AveragePooling2D, AveragePooling1D, UpSampling1D, UpSampling2D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "def get_optimizador():\n",
    "    adam = Adam(learning_rate=1e-5)\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, RemoteMonitor, TerminateOnNaN, BackupAndRestore\n",
    "\n",
    "def get_callbacks(name=\"model\"):\n",
    "    #EarlyStopping, detener el entrenamiento una vez que su pérdida comienza a aumentar\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='accuracy',\n",
    "        patience=8, #argumento de patience representa el número de épocas antes de detenerse una vez que su pérdida comienza a aumentar (deja de mejorar).\n",
    "        min_delta=0,  #es un umbral para cuantificar una pérdida en alguna época como mejora o no. Si la diferencia de pérdida es inferior a min_delta , se cuantifica como no mejora. Es mejor dejarlo como 0 ya que estamos interesados ​​en cuando la pérdida empeora.\n",
    "        restore_best_weights=True,\n",
    "        mode='max')\n",
    "\n",
    "    #ReduceLROnPlateau, que si el entrenamiento no mejora tras unos epochs específicos, reduce el valor de learning rate del modelo\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='loss', \n",
    "        factor=0.1, \n",
    "        patience=5, \n",
    "        min_delta=1e-4, \n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Saves Keras model after each epoch\n",
    "    #Para algunos casos es importante saber cual entrenamiento fue mejor, \n",
    "    #este callback guarda el modelo tras cada epoca completada con el fin de si luego se desea un registro de pesos para cada epoca\n",
    "    #Se ha usado este callback para poder optener el mejor modelo de pesos, sobretodo en la red neuronal creada desde cero\n",
    "    #siendo de gran utilidad para determinar el como ir modificando los layer hasta obtener el mejor modelo\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath='models_backup/' + name +'-{val_accuracy:.4f}.h5', \n",
    "        monitor='val_accuracy',\n",
    "        verbose=1, \n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False\n",
    "    )\n",
    "\n",
    "    remote_monitor = RemoteMonitor(\n",
    "        root='http://localhost:6006',\n",
    "        path='/publish/epoch/end/',\n",
    "        field='data',\n",
    "        headers=None,\n",
    "        send_as_json=False\n",
    "    )\n",
    "    \n",
    "    backup_restore = BackupAndRestore(backup_dir=\"backup\")\n",
    "    \n",
    "    proteccion_nan_loss = TerminateOnNaN()\n",
    "\n",
    "    \n",
    "    callbacks_list = [early_stop, reduce_lr, checkpointer, proteccion_nan_loss, backup_restore]#, remote_monitor]\n",
    "    \n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Modelo basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RedBasica\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                512       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               16640     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 11)                363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,443\n",
      "Trainable params: 122,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cantidad_entradas = 15\n",
    "cantidad_salidas = 11\n",
    "\n",
    "model = Sequential(name=\"RedBasica\")\n",
    "model.add(Dense(32, activation='relu', input_shape=(cantidad_entradas,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(cantidad_salidas, activation='softmax', name='output_layer'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=get_optimizador(),\n",
    "              loss='mse',  # categorical_crossentropy sparse_categorical_crossentropy mean_squared_error\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.1435 - accuracy: 0.1979\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50800, saving model to models_backup/model-0.5080.h5\n",
      "572/572 [==============================] - 3s 3ms/step - loss: 0.1435 - accuracy: 0.1992 - val_loss: 0.1379 - val_accuracy: 0.5080 - lr: 1.0000e-05\n",
      "Epoch 2/500\n",
      "556/572 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.4542\n",
      "Epoch 2: val_accuracy improved from 0.50800 to 0.57224, saving model to models_backup/model-0.5722.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.1275 - accuracy: 0.4560 - val_loss: 0.1127 - val_accuracy: 0.5722 - lr: 1.0000e-05\n",
      "Epoch 3/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.5389\n",
      "Epoch 3: val_accuracy improved from 0.57224 to 0.67192, saving model to models_backup/model-0.6719.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.1139 - accuracy: 0.5395 - val_loss: 0.1002 - val_accuracy: 0.6719 - lr: 1.0000e-05\n",
      "Epoch 4/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.5896\n",
      "Epoch 4: val_accuracy improved from 0.67192 to 0.71007, saving model to models_backup/model-0.7101.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.1073 - accuracy: 0.5894 - val_loss: 0.0963 - val_accuracy: 0.7101 - lr: 1.0000e-05\n",
      "Epoch 5/500\n",
      "562/572 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.6222\n",
      "Epoch 5: val_accuracy improved from 0.71007 to 0.71843, saving model to models_backup/model-0.7184.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.1032 - accuracy: 0.6219 - val_loss: 0.0943 - val_accuracy: 0.7184 - lr: 1.0000e-05\n",
      "Epoch 6/500\n",
      "562/572 [============================>.] - ETA: 0s - loss: 0.1005 - accuracy: 0.6409\n",
      "Epoch 6: val_accuracy improved from 0.71843 to 0.73049, saving model to models_backup/model-0.7305.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.1005 - accuracy: 0.6407 - val_loss: 0.0929 - val_accuracy: 0.7305 - lr: 1.0000e-05\n",
      "Epoch 7/500\n",
      "550/572 [===========================>..] - ETA: 0s - loss: 0.0983 - accuracy: 0.6513\n",
      "Epoch 7: val_accuracy improved from 0.73049 to 0.73837, saving model to models_backup/model-0.7384.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0982 - accuracy: 0.6518 - val_loss: 0.0919 - val_accuracy: 0.7384 - lr: 1.0000e-05\n",
      "Epoch 8/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0965 - accuracy: 0.6606\n",
      "Epoch 8: val_accuracy improved from 0.73837 to 0.73886, saving model to models_backup/model-0.7389.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0964 - accuracy: 0.6610 - val_loss: 0.0911 - val_accuracy: 0.7389 - lr: 1.0000e-05\n",
      "Epoch 9/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.6619\n",
      "Epoch 9: val_accuracy improved from 0.73886 to 0.74698, saving model to models_backup/model-0.7470.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0953 - accuracy: 0.6624 - val_loss: 0.0903 - val_accuracy: 0.7470 - lr: 1.0000e-05\n",
      "Epoch 10/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.6690\n",
      "Epoch 10: val_accuracy did not improve from 0.74698\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0941 - accuracy: 0.6688 - val_loss: 0.0897 - val_accuracy: 0.7435 - lr: 1.0000e-05\n",
      "Epoch 11/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.6734\n",
      "Epoch 11: val_accuracy improved from 0.74698 to 0.75437, saving model to models_backup/model-0.7544.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0929 - accuracy: 0.6737 - val_loss: 0.0892 - val_accuracy: 0.7544 - lr: 1.0000e-05\n",
      "Epoch 12/500\n",
      "551/572 [===========================>..] - ETA: 0s - loss: 0.0920 - accuracy: 0.6769\n",
      "Epoch 12: val_accuracy did not improve from 0.75437\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0921 - accuracy: 0.6773 - val_loss: 0.0883 - val_accuracy: 0.7475 - lr: 1.0000e-05\n",
      "Epoch 13/500\n",
      "561/572 [============================>.] - ETA: 0s - loss: 0.0912 - accuracy: 0.6771\n",
      "Epoch 13: val_accuracy improved from 0.75437 to 0.75461, saving model to models_backup/model-0.7546.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0912 - accuracy: 0.6768 - val_loss: 0.0875 - val_accuracy: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 14/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.6776\n",
      "Epoch 14: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0904 - accuracy: 0.6771 - val_loss: 0.0867 - val_accuracy: 0.7512 - lr: 1.0000e-05\n",
      "Epoch 15/500\n",
      "555/572 [============================>.] - ETA: 0s - loss: 0.0899 - accuracy: 0.6774\n",
      "Epoch 15: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0898 - accuracy: 0.6776 - val_loss: 0.0858 - val_accuracy: 0.7534 - lr: 1.0000e-05\n",
      "Epoch 16/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0890 - accuracy: 0.6799\n",
      "Epoch 16: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0889 - accuracy: 0.6800 - val_loss: 0.0850 - val_accuracy: 0.7371 - lr: 1.0000e-05\n",
      "Epoch 17/500\n",
      "559/572 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.6787\n",
      "Epoch 17: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.6782 - val_loss: 0.0842 - val_accuracy: 0.7517 - lr: 1.0000e-05\n",
      "Epoch 18/500\n",
      "555/572 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.6788\n",
      "Epoch 18: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0875 - accuracy: 0.6785 - val_loss: 0.0837 - val_accuracy: 0.7529 - lr: 1.0000e-05\n",
      "Epoch 19/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0867 - accuracy: 0.6857\n",
      "Epoch 19: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0867 - accuracy: 0.6856 - val_loss: 0.0830 - val_accuracy: 0.7487 - lr: 1.0000e-05\n",
      "Epoch 20/500\n",
      "563/572 [============================>.] - ETA: 0s - loss: 0.0860 - accuracy: 0.6859\n",
      "Epoch 20: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0860 - accuracy: 0.6858 - val_loss: 0.0825 - val_accuracy: 0.7477 - lr: 1.0000e-05\n",
      "Epoch 21/500\n",
      "561/572 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.6868\n",
      "Epoch 21: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0854 - accuracy: 0.6869 - val_loss: 0.0821 - val_accuracy: 0.7467 - lr: 1.0000e-05\n",
      "Epoch 22/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.6879\n",
      "Epoch 22: val_accuracy did not improve from 0.75461\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0848 - accuracy: 0.6884 - val_loss: 0.0816 - val_accuracy: 0.7470 - lr: 1.0000e-05\n",
      "Epoch 23/500\n",
      "559/572 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.6900\n",
      "Epoch 23: val_accuracy improved from 0.75461 to 0.75880, saving model to models_backup/model-0.7588.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0840 - accuracy: 0.6903 - val_loss: 0.0813 - val_accuracy: 0.7588 - lr: 1.0000e-05\n",
      "Epoch 24/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0836 - accuracy: 0.6952\n",
      "Epoch 24: val_accuracy did not improve from 0.75880\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0836 - accuracy: 0.6952 - val_loss: 0.0808 - val_accuracy: 0.7554 - lr: 1.0000e-05\n",
      "Epoch 25/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.6947\n",
      "Epoch 25: val_accuracy did not improve from 0.75880\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0831 - accuracy: 0.6946 - val_loss: 0.0803 - val_accuracy: 0.7578 - lr: 1.0000e-05\n",
      "Epoch 26/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.6976\n",
      "Epoch 26: val_accuracy did not improve from 0.75880\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0824 - accuracy: 0.6976 - val_loss: 0.0800 - val_accuracy: 0.7586 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/500\n",
      "550/572 [===========================>..] - ETA: 0s - loss: 0.0819 - accuracy: 0.7004\n",
      "Epoch 27: val_accuracy did not improve from 0.75880\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0820 - accuracy: 0.7004 - val_loss: 0.0795 - val_accuracy: 0.7529 - lr: 1.0000e-05\n",
      "Epoch 28/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.7035\n",
      "Epoch 28: val_accuracy improved from 0.75880 to 0.75978, saving model to models_backup/model-0.7598.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0813 - accuracy: 0.7034 - val_loss: 0.0791 - val_accuracy: 0.7598 - lr: 1.0000e-05\n",
      "Epoch 29/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.7007\n",
      "Epoch 29: val_accuracy improved from 0.75978 to 0.76815, saving model to models_backup/model-0.7682.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0809 - accuracy: 0.7012 - val_loss: 0.0789 - val_accuracy: 0.7682 - lr: 1.0000e-05\n",
      "Epoch 30/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.7028\n",
      "Epoch 30: val_accuracy improved from 0.76815 to 0.77258, saving model to models_backup/model-0.7726.h5\n",
      "572/572 [==============================] - 1s 3ms/step - loss: 0.0804 - accuracy: 0.7028 - val_loss: 0.0785 - val_accuracy: 0.7726 - lr: 1.0000e-05\n",
      "Epoch 31/500\n",
      "548/572 [===========================>..] - ETA: 0s - loss: 0.0799 - accuracy: 0.7045\n",
      "Epoch 31: val_accuracy did not improve from 0.77258\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0800 - accuracy: 0.7047 - val_loss: 0.0783 - val_accuracy: 0.7709 - lr: 1.0000e-05\n",
      "Epoch 32/500\n",
      "551/572 [===========================>..] - ETA: 0s - loss: 0.0794 - accuracy: 0.7091\n",
      "Epoch 32: val_accuracy improved from 0.77258 to 0.77603, saving model to models_backup/model-0.7760.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0796 - accuracy: 0.7079 - val_loss: 0.0779 - val_accuracy: 0.7760 - lr: 1.0000e-05\n",
      "Epoch 33/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.7086\n",
      "Epoch 33: val_accuracy did not improve from 0.77603\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0792 - accuracy: 0.7086 - val_loss: 0.0777 - val_accuracy: 0.7743 - lr: 1.0000e-05\n",
      "Epoch 34/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.7076\n",
      "Epoch 34: val_accuracy improved from 0.77603 to 0.78587, saving model to models_backup/model-0.7859.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.7076 - val_loss: 0.0776 - val_accuracy: 0.7859 - lr: 1.0000e-05\n",
      "Epoch 35/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.7099\n",
      "Epoch 35: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0784 - accuracy: 0.7098 - val_loss: 0.0772 - val_accuracy: 0.7775 - lr: 1.0000e-05\n",
      "Epoch 36/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0782 - accuracy: 0.7111\n",
      "Epoch 36: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0782 - accuracy: 0.7111 - val_loss: 0.0770 - val_accuracy: 0.7755 - lr: 1.0000e-05\n",
      "Epoch 37/500\n",
      "561/572 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.7123\n",
      "Epoch 37: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0778 - accuracy: 0.7121 - val_loss: 0.0767 - val_accuracy: 0.7782 - lr: 1.0000e-05\n",
      "Epoch 38/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0777 - accuracy: 0.7145\n",
      "Epoch 38: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0776 - accuracy: 0.7147 - val_loss: 0.0765 - val_accuracy: 0.7778 - lr: 1.0000e-05\n",
      "Epoch 39/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.7118\n",
      "Epoch 39: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0772 - accuracy: 0.7118 - val_loss: 0.0763 - val_accuracy: 0.7780 - lr: 1.0000e-05\n",
      "Epoch 40/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.7150\n",
      "Epoch 40: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.7150 - val_loss: 0.0761 - val_accuracy: 0.7782 - lr: 1.0000e-05\n",
      "Epoch 41/500\n",
      "555/572 [============================>.] - ETA: 0s - loss: 0.0767 - accuracy: 0.7153\n",
      "Epoch 41: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0767 - accuracy: 0.7155 - val_loss: 0.0759 - val_accuracy: 0.7834 - lr: 1.0000e-05\n",
      "Epoch 42/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.7179\n",
      "Epoch 42: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0765 - accuracy: 0.7178 - val_loss: 0.0758 - val_accuracy: 0.7839 - lr: 1.0000e-05\n",
      "Epoch 43/500\n",
      "559/572 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.7153\n",
      "Epoch 43: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0761 - accuracy: 0.7156 - val_loss: 0.0756 - val_accuracy: 0.7805 - lr: 1.0000e-05\n",
      "Epoch 44/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.7167\n",
      "Epoch 44: val_accuracy did not improve from 0.78587\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0759 - accuracy: 0.7164 - val_loss: 0.0753 - val_accuracy: 0.7800 - lr: 1.0000e-05\n",
      "Epoch 45/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0757 - accuracy: 0.7166\n",
      "Epoch 45: val_accuracy improved from 0.78587 to 0.78760, saving model to models_backup/model-0.7876.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.7165 - val_loss: 0.0752 - val_accuracy: 0.7876 - lr: 1.0000e-05\n",
      "Epoch 46/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.7182\n",
      "Epoch 46: val_accuracy did not improve from 0.78760\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0755 - accuracy: 0.7181 - val_loss: 0.0750 - val_accuracy: 0.7770 - lr: 1.0000e-05\n",
      "Epoch 47/500\n",
      "555/572 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.7172\n",
      "Epoch 47: val_accuracy did not improve from 0.78760\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0754 - accuracy: 0.7173 - val_loss: 0.0748 - val_accuracy: 0.7814 - lr: 1.0000e-05\n",
      "Epoch 48/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.7207\n",
      "Epoch 48: val_accuracy did not improve from 0.78760\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0751 - accuracy: 0.7207 - val_loss: 0.0747 - val_accuracy: 0.7859 - lr: 1.0000e-05\n",
      "Epoch 49/500\n",
      "560/572 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.7203\n",
      "Epoch 49: val_accuracy did not improve from 0.78760\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0748 - accuracy: 0.7204 - val_loss: 0.0745 - val_accuracy: 0.7849 - lr: 1.0000e-05\n",
      "Epoch 50/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.7236\n",
      "Epoch 50: val_accuracy improved from 0.78760 to 0.79227, saving model to models_backup/model-0.7923.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0747 - accuracy: 0.7229 - val_loss: 0.0744 - val_accuracy: 0.7923 - lr: 1.0000e-05\n",
      "Epoch 51/500\n",
      "562/572 [============================>.] - ETA: 0s - loss: 0.0747 - accuracy: 0.7219\n",
      "Epoch 51: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0745 - accuracy: 0.7226 - val_loss: 0.0742 - val_accuracy: 0.7780 - lr: 1.0000e-05\n",
      "Epoch 52/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.7259\n",
      "Epoch 52: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0743 - accuracy: 0.7256 - val_loss: 0.0741 - val_accuracy: 0.7792 - lr: 1.0000e-05\n",
      "Epoch 53/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.7249\n",
      "Epoch 53: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0740 - accuracy: 0.7249 - val_loss: 0.0740 - val_accuracy: 0.7905 - lr: 1.0000e-05\n",
      "Epoch 54/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0740 - accuracy: 0.7275\n",
      "Epoch 54: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0739 - accuracy: 0.7276 - val_loss: 0.0738 - val_accuracy: 0.7709 - lr: 1.0000e-05\n",
      "Epoch 55/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.7228\n",
      "Epoch 55: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.7229 - val_loss: 0.0737 - val_accuracy: 0.7785 - lr: 1.0000e-05\n",
      "Epoch 56/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.7270\n",
      "Epoch 56: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0737 - accuracy: 0.7269 - val_loss: 0.0735 - val_accuracy: 0.7871 - lr: 1.0000e-05\n",
      "Epoch 57/500\n",
      "548/572 [===========================>..] - ETA: 0s - loss: 0.0736 - accuracy: 0.7269\n",
      "Epoch 57: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0735 - accuracy: 0.7276 - val_loss: 0.0735 - val_accuracy: 0.7704 - lr: 1.0000e-05\n",
      "Epoch 58/500\n",
      "561/572 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.7259\n",
      "Epoch 58: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.7256 - val_loss: 0.0733 - val_accuracy: 0.7802 - lr: 1.0000e-05\n",
      "Epoch 59/500\n",
      "551/572 [===========================>..] - ETA: 0s - loss: 0.0734 - accuracy: 0.7288\n",
      "Epoch 59: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.7287 - val_loss: 0.0733 - val_accuracy: 0.7755 - lr: 1.0000e-05\n",
      "Epoch 60/500\n",
      "548/572 [===========================>..] - ETA: 0s - loss: 0.0731 - accuracy: 0.7283\n",
      "Epoch 60: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0731 - accuracy: 0.7288 - val_loss: 0.0732 - val_accuracy: 0.7812 - lr: 1.0000e-05\n",
      "Epoch 61/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.7281\n",
      "Epoch 61: val_accuracy did not improve from 0.79227\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0730 - accuracy: 0.7285 - val_loss: 0.0730 - val_accuracy: 0.7893 - lr: 1.0000e-05\n",
      "Epoch 62/500\n",
      "557/572 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.7287\n",
      "Epoch 62: val_accuracy improved from 0.79227 to 0.79350, saving model to models_backup/model-0.7935.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0729 - accuracy: 0.7291 - val_loss: 0.0730 - val_accuracy: 0.7935 - lr: 1.0000e-05\n",
      "Epoch 63/500\n",
      "549/572 [===========================>..] - ETA: 0s - loss: 0.0726 - accuracy: 0.7328\n",
      "Epoch 63: val_accuracy did not improve from 0.79350\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.7325 - val_loss: 0.0728 - val_accuracy: 0.7770 - lr: 1.0000e-05\n",
      "Epoch 64/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.7302\n",
      "Epoch 64: val_accuracy did not improve from 0.79350\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.7305 - val_loss: 0.0727 - val_accuracy: 0.7817 - lr: 1.0000e-05\n",
      "Epoch 65/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0725 - accuracy: 0.7314\n",
      "Epoch 65: val_accuracy did not improve from 0.79350\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0725 - accuracy: 0.7313 - val_loss: 0.0727 - val_accuracy: 0.7869 - lr: 1.0000e-05\n",
      "Epoch 66/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.7297\n",
      "Epoch 66: val_accuracy did not improve from 0.79350\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0723 - accuracy: 0.7299 - val_loss: 0.0726 - val_accuracy: 0.7750 - lr: 1.0000e-05\n",
      "Epoch 67/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0723 - accuracy: 0.7312\n",
      "Epoch 67: val_accuracy did not improve from 0.79350\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0724 - accuracy: 0.7310 - val_loss: 0.0725 - val_accuracy: 0.7775 - lr: 1.0000e-05\n",
      "Epoch 68/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.7347\n",
      "Epoch 68: val_accuracy did not improve from 0.79350\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0722 - accuracy: 0.7347 - val_loss: 0.0725 - val_accuracy: 0.7765 - lr: 1.0000e-05\n",
      "Epoch 69/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.7351\n",
      "Epoch 69: val_accuracy did not improve from 0.79350\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0720 - accuracy: 0.7353 - val_loss: 0.0723 - val_accuracy: 0.7864 - lr: 1.0000e-05\n",
      "Epoch 70/500\n",
      "554/572 [============================>.] - ETA: 0s - loss: 0.0720 - accuracy: 0.7325\n",
      "Epoch 70: val_accuracy improved from 0.79350 to 0.79449, saving model to models_backup/model-0.7945.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.7328 - val_loss: 0.0724 - val_accuracy: 0.7945 - lr: 1.0000e-05\n",
      "Epoch 71/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.7355\n",
      "Epoch 71: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0718 - accuracy: 0.7353 - val_loss: 0.0722 - val_accuracy: 0.7753 - lr: 1.0000e-05\n",
      "Epoch 72/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.7356\n",
      "Epoch 72: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0717 - accuracy: 0.7357 - val_loss: 0.0720 - val_accuracy: 0.7733 - lr: 1.0000e-05\n",
      "Epoch 73/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.7355\n",
      "Epoch 73: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0716 - accuracy: 0.7354 - val_loss: 0.0720 - val_accuracy: 0.7822 - lr: 1.0000e-05\n",
      "Epoch 74/500\n",
      "551/572 [===========================>..] - ETA: 0s - loss: 0.0716 - accuracy: 0.7364\n",
      "Epoch 74: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0715 - accuracy: 0.7365 - val_loss: 0.0719 - val_accuracy: 0.7738 - lr: 1.0000e-05\n",
      "Epoch 75/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.7366\n",
      "Epoch 75: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0714 - accuracy: 0.7370 - val_loss: 0.0718 - val_accuracy: 0.7763 - lr: 1.0000e-05\n",
      "Epoch 76/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.7384\n",
      "Epoch 76: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0713 - accuracy: 0.7380 - val_loss: 0.0718 - val_accuracy: 0.7839 - lr: 1.0000e-05\n",
      "Epoch 77/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.7393\n",
      "Epoch 77: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0712 - accuracy: 0.7390 - val_loss: 0.0718 - val_accuracy: 0.7810 - lr: 1.0000e-05\n",
      "Epoch 78/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.7401\n",
      "Epoch 78: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0711 - accuracy: 0.7396 - val_loss: 0.0718 - val_accuracy: 0.7905 - lr: 1.0000e-05\n",
      "Epoch 79/500\n",
      "561/572 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.7408\n",
      "Epoch 79: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0710 - accuracy: 0.7411 - val_loss: 0.0716 - val_accuracy: 0.7819 - lr: 1.0000e-05\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571/572 [============================>.] - ETA: 0s - loss: 0.0710 - accuracy: 0.7419\n",
      "Epoch 80: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0710 - accuracy: 0.7419 - val_loss: 0.0716 - val_accuracy: 0.7581 - lr: 1.0000e-05\n",
      "Epoch 81/500\n",
      "556/572 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.7407\n",
      "Epoch 81: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0709 - accuracy: 0.7403 - val_loss: 0.0715 - val_accuracy: 0.7770 - lr: 1.0000e-05\n",
      "Epoch 82/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.7382\n",
      "Epoch 82: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0709 - accuracy: 0.7382 - val_loss: 0.0715 - val_accuracy: 0.7844 - lr: 1.0000e-05\n",
      "Epoch 83/500\n",
      "554/572 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.7382\n",
      "Epoch 83: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0708 - accuracy: 0.7387 - val_loss: 0.0715 - val_accuracy: 0.7903 - lr: 1.0000e-05\n",
      "Epoch 84/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.7423\n",
      "Epoch 84: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0707 - accuracy: 0.7426 - val_loss: 0.0714 - val_accuracy: 0.7893 - lr: 1.0000e-05\n",
      "Epoch 85/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.7418\n",
      "Epoch 85: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0706 - accuracy: 0.7416 - val_loss: 0.0714 - val_accuracy: 0.7758 - lr: 1.0000e-05\n",
      "Epoch 86/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.7417\n",
      "Epoch 86: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0706 - accuracy: 0.7417 - val_loss: 0.0714 - val_accuracy: 0.7903 - lr: 1.0000e-05\n",
      "Epoch 87/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.7422\n",
      "Epoch 87: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0705 - accuracy: 0.7425 - val_loss: 0.0712 - val_accuracy: 0.7844 - lr: 1.0000e-05\n",
      "Epoch 88/500\n",
      "550/572 [===========================>..] - ETA: 0s - loss: 0.0705 - accuracy: 0.7463\n",
      "Epoch 88: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0705 - accuracy: 0.7469 - val_loss: 0.0713 - val_accuracy: 0.7864 - lr: 1.0000e-05\n",
      "Epoch 89/500\n",
      "563/572 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.7434\n",
      "Epoch 89: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0703 - accuracy: 0.7440 - val_loss: 0.0712 - val_accuracy: 0.7873 - lr: 1.0000e-05\n",
      "Epoch 90/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.7466\n",
      "Epoch 90: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0704 - accuracy: 0.7466 - val_loss: 0.0711 - val_accuracy: 0.7834 - lr: 1.0000e-05\n",
      "Epoch 91/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.7488\n",
      "Epoch 91: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0702 - accuracy: 0.7488 - val_loss: 0.0711 - val_accuracy: 0.7790 - lr: 1.0000e-05\n",
      "Epoch 92/500\n",
      "552/572 [===========================>..] - ETA: 0s - loss: 0.0702 - accuracy: 0.7435\n",
      "Epoch 92: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0701 - accuracy: 0.7434 - val_loss: 0.0711 - val_accuracy: 0.7854 - lr: 1.0000e-05\n",
      "Epoch 93/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.7462\n",
      "Epoch 93: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0701 - accuracy: 0.7464 - val_loss: 0.0711 - val_accuracy: 0.7881 - lr: 1.0000e-05\n",
      "Epoch 94/500\n",
      "548/572 [===========================>..] - ETA: 0s - loss: 0.0701 - accuracy: 0.7494\n",
      "Epoch 94: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0701 - accuracy: 0.7492 - val_loss: 0.0710 - val_accuracy: 0.7770 - lr: 1.0000e-05\n",
      "Epoch 95/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.7473\n",
      "Epoch 95: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0700 - accuracy: 0.7473 - val_loss: 0.0710 - val_accuracy: 0.7893 - lr: 1.0000e-05\n",
      "Epoch 96/500\n",
      "563/572 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.7497\n",
      "Epoch 96: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0700 - accuracy: 0.7494 - val_loss: 0.0709 - val_accuracy: 0.7760 - lr: 1.0000e-05\n",
      "Epoch 97/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.7492\n",
      "Epoch 97: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0699 - accuracy: 0.7492 - val_loss: 0.0709 - val_accuracy: 0.7866 - lr: 1.0000e-05\n",
      "Epoch 98/500\n",
      "560/572 [============================>.] - ETA: 0s - loss: 0.0699 - accuracy: 0.7482\n",
      "Epoch 98: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0698 - accuracy: 0.7482 - val_loss: 0.0708 - val_accuracy: 0.7910 - lr: 1.0000e-05\n",
      "Epoch 99/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0697 - accuracy: 0.7515\n",
      "Epoch 99: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0697 - accuracy: 0.7518 - val_loss: 0.0708 - val_accuracy: 0.7920 - lr: 1.0000e-05\n",
      "Epoch 100/500\n",
      "550/572 [===========================>..] - ETA: 0s - loss: 0.0698 - accuracy: 0.7531\n",
      "Epoch 100: val_accuracy did not improve from 0.79449\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0698 - accuracy: 0.7525 - val_loss: 0.0707 - val_accuracy: 0.7824 - lr: 1.0000e-05\n",
      "Epoch 101/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.7474\n",
      "Epoch 101: val_accuracy improved from 0.79449 to 0.79473, saving model to models_backup/model-0.7947.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0697 - accuracy: 0.7471 - val_loss: 0.0708 - val_accuracy: 0.7947 - lr: 1.0000e-05\n",
      "Epoch 102/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.7472\n",
      "Epoch 102: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0696 - accuracy: 0.7475 - val_loss: 0.0707 - val_accuracy: 0.7925 - lr: 1.0000e-05\n",
      "Epoch 103/500\n",
      "551/572 [===========================>..] - ETA: 0s - loss: 0.0696 - accuracy: 0.7517\n",
      "Epoch 103: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0695 - accuracy: 0.7518 - val_loss: 0.0707 - val_accuracy: 0.7930 - lr: 1.0000e-05\n",
      "Epoch 104/500\n",
      "549/572 [===========================>..] - ETA: 0s - loss: 0.0696 - accuracy: 0.7508\n",
      "Epoch 104: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0696 - accuracy: 0.7512 - val_loss: 0.0706 - val_accuracy: 0.7910 - lr: 1.0000e-05\n",
      "Epoch 105/500\n",
      "562/572 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.7557\n",
      "Epoch 105: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0695 - accuracy: 0.7552 - val_loss: 0.0706 - val_accuracy: 0.7871 - lr: 1.0000e-05\n",
      "Epoch 106/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.7525\n",
      "Epoch 106: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0694 - accuracy: 0.7526 - val_loss: 0.0706 - val_accuracy: 0.7846 - lr: 1.0000e-05\n",
      "Epoch 107/500\n",
      "553/572 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.7551\n",
      "Epoch 107: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0693 - accuracy: 0.7555 - val_loss: 0.0705 - val_accuracy: 0.7925 - lr: 1.0000e-05\n",
      "Epoch 108/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0694 - accuracy: 0.7561\n",
      "Epoch 108: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0693 - accuracy: 0.7566 - val_loss: 0.0705 - val_accuracy: 0.7918 - lr: 1.0000e-05\n",
      "Epoch 109/500\n",
      "551/572 [===========================>..] - ETA: 0s - loss: 0.0692 - accuracy: 0.7594\n",
      "Epoch 109: val_accuracy did not improve from 0.79473\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0693 - accuracy: 0.7592 - val_loss: 0.0705 - val_accuracy: 0.7829 - lr: 1.0000e-05\n",
      "Epoch 110/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0692 - accuracy: 0.7552\n",
      "Epoch 110: val_accuracy improved from 0.79473 to 0.79966, saving model to models_backup/model-0.7997.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0692 - accuracy: 0.7554 - val_loss: 0.0705 - val_accuracy: 0.7997 - lr: 1.0000e-05\n",
      "Epoch 111/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0692 - accuracy: 0.7600\n",
      "Epoch 111: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.7599 - val_loss: 0.0704 - val_accuracy: 0.7873 - lr: 1.0000e-05\n",
      "Epoch 112/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0692 - accuracy: 0.7581\n",
      "Epoch 112: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.7579 - val_loss: 0.0704 - val_accuracy: 0.7888 - lr: 1.0000e-05\n",
      "Epoch 113/500\n",
      "561/572 [============================>.] - ETA: 0s - loss: 0.0691 - accuracy: 0.7613\n",
      "Epoch 113: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.7608 - val_loss: 0.0704 - val_accuracy: 0.7873 - lr: 1.0000e-05\n",
      "Epoch 114/500\n",
      "550/572 [===========================>..] - ETA: 0s - loss: 0.0690 - accuracy: 0.7622\n",
      "Epoch 114: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0690 - accuracy: 0.7617 - val_loss: 0.0704 - val_accuracy: 0.7871 - lr: 1.0000e-05\n",
      "Epoch 115/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.7621\n",
      "Epoch 115: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0690 - accuracy: 0.7622 - val_loss: 0.0703 - val_accuracy: 0.7942 - lr: 1.0000e-05\n",
      "Epoch 116/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.7635\n",
      "Epoch 116: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0688 - accuracy: 0.7635 - val_loss: 0.0702 - val_accuracy: 0.7933 - lr: 1.0000e-05\n",
      "Epoch 117/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.7623\n",
      "Epoch 117: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0689 - accuracy: 0.7625 - val_loss: 0.0702 - val_accuracy: 0.7873 - lr: 1.0000e-05\n",
      "Epoch 118/500\n",
      "562/572 [============================>.] - ETA: 0s - loss: 0.0689 - accuracy: 0.7601\n",
      "Epoch 118: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0689 - accuracy: 0.7604 - val_loss: 0.0702 - val_accuracy: 0.7972 - lr: 1.0000e-05\n",
      "Epoch 119/500\n",
      "563/572 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.7660\n",
      "Epoch 119: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0688 - accuracy: 0.7661 - val_loss: 0.0702 - val_accuracy: 0.7928 - lr: 1.0000e-05\n",
      "Epoch 120/500\n",
      "563/572 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.7612\n",
      "Epoch 120: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0688 - accuracy: 0.7614 - val_loss: 0.0702 - val_accuracy: 0.7928 - lr: 1.0000e-05\n",
      "Epoch 121/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.7644\n",
      "Epoch 121: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0687 - accuracy: 0.7642 - val_loss: 0.0702 - val_accuracy: 0.7915 - lr: 1.0000e-05\n",
      "Epoch 122/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0686 - accuracy: 0.7644\n",
      "Epoch 122: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0686 - accuracy: 0.7644 - val_loss: 0.0701 - val_accuracy: 0.7979 - lr: 1.0000e-05\n",
      "Epoch 123/500\n",
      "548/572 [===========================>..] - ETA: 0s - loss: 0.0687 - accuracy: 0.7659\n",
      "Epoch 123: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0686 - accuracy: 0.7660 - val_loss: 0.0701 - val_accuracy: 0.7945 - lr: 1.0000e-05\n",
      "Epoch 124/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.7696\n",
      "Epoch 124: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0686 - accuracy: 0.7702 - val_loss: 0.0701 - val_accuracy: 0.7856 - lr: 1.0000e-05\n",
      "Epoch 125/500\n",
      "564/572 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.7669\n",
      "Epoch 125: val_accuracy did not improve from 0.79966\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.7675 - val_loss: 0.0700 - val_accuracy: 0.7846 - lr: 1.0000e-05\n",
      "Epoch 126/500\n",
      "548/572 [===========================>..] - ETA: 0s - loss: 0.0686 - accuracy: 0.7657\n",
      "Epoch 126: val_accuracy improved from 0.79966 to 0.80015, saving model to models_backup/model-0.8001.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.7662 - val_loss: 0.0701 - val_accuracy: 0.8001 - lr: 1.0000e-05\n",
      "Epoch 127/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.7690\n",
      "Epoch 127: val_accuracy did not improve from 0.80015\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.7688 - val_loss: 0.0700 - val_accuracy: 0.7866 - lr: 1.0000e-05\n",
      "Epoch 128/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.7680\n",
      "Epoch 128: val_accuracy did not improve from 0.80015\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0685 - accuracy: 0.7679 - val_loss: 0.0700 - val_accuracy: 0.7940 - lr: 1.0000e-05\n",
      "Epoch 129/500\n",
      "559/572 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.7702\n",
      "Epoch 129: val_accuracy did not improve from 0.80015\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.7703 - val_loss: 0.0700 - val_accuracy: 0.7854 - lr: 1.0000e-05\n",
      "Epoch 130/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0685 - accuracy: 0.7725\n",
      "Epoch 130: val_accuracy did not improve from 0.80015\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.7725 - val_loss: 0.0700 - val_accuracy: 0.7886 - lr: 1.0000e-05\n",
      "Epoch 131/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.7712\n",
      "Epoch 131: val_accuracy improved from 0.80015 to 0.80605, saving model to models_backup/model-0.8061.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.7712 - val_loss: 0.0700 - val_accuracy: 0.8061 - lr: 1.0000e-05\n",
      "Epoch 132/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.7721\n",
      "Epoch 132: val_accuracy did not improve from 0.80605\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0683 - accuracy: 0.7724 - val_loss: 0.0699 - val_accuracy: 0.7930 - lr: 1.0000e-05\n",
      "Epoch 133/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.7764\n",
      "Epoch 133: val_accuracy did not improve from 0.80605\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0682 - accuracy: 0.7764 - val_loss: 0.0699 - val_accuracy: 0.7962 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.7767\n",
      "Epoch 134: val_accuracy did not improve from 0.80605\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0682 - accuracy: 0.7768 - val_loss: 0.0699 - val_accuracy: 0.8024 - lr: 1.0000e-05\n",
      "Epoch 135/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.7764\n",
      "Epoch 135: val_accuracy did not improve from 0.80605\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0682 - accuracy: 0.7764 - val_loss: 0.0699 - val_accuracy: 0.8031 - lr: 1.0000e-05\n",
      "Epoch 136/500\n",
      "560/572 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.7766\n",
      "Epoch 136: val_accuracy did not improve from 0.80605\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0682 - accuracy: 0.7767 - val_loss: 0.0698 - val_accuracy: 0.8014 - lr: 1.0000e-05\n",
      "Epoch 137/500\n",
      "549/572 [===========================>..] - ETA: 0s - loss: 0.0681 - accuracy: 0.7776\n",
      "Epoch 137: val_accuracy did not improve from 0.80605\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0682 - accuracy: 0.7772 - val_loss: 0.0698 - val_accuracy: 0.8009 - lr: 1.0000e-05\n",
      "Epoch 138/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.7776\n",
      "Epoch 138: val_accuracy did not improve from 0.80605\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.7780 - val_loss: 0.0698 - val_accuracy: 0.7984 - lr: 1.0000e-05\n",
      "Epoch 139/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.7762\n",
      "Epoch 139: val_accuracy improved from 0.80605 to 0.80999, saving model to models_backup/model-0.8100.h5\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0681 - accuracy: 0.7762 - val_loss: 0.0697 - val_accuracy: 0.8100 - lr: 1.0000e-05\n",
      "Epoch 140/500\n",
      "562/572 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.7796\n",
      "Epoch 140: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.7796 - val_loss: 0.0698 - val_accuracy: 0.8033 - lr: 1.0000e-05\n",
      "Epoch 141/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.7776\n",
      "Epoch 141: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.7774 - val_loss: 0.0698 - val_accuracy: 0.8048 - lr: 1.0000e-05\n",
      "Epoch 142/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.7831\n",
      "Epoch 142: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.7829 - val_loss: 0.0697 - val_accuracy: 0.7999 - lr: 1.0000e-05\n",
      "Epoch 143/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.7837\n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 143: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.7837 - val_loss: 0.0697 - val_accuracy: 0.8014 - lr: 1.0000e-05\n",
      "Epoch 144/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.7822\n",
      "Epoch 144: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7823 - val_loss: 0.0697 - val_accuracy: 0.8024 - lr: 1.0000e-06\n",
      "Epoch 145/500\n",
      "563/572 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.7840\n",
      "Epoch 145: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7837 - val_loss: 0.0697 - val_accuracy: 0.8036 - lr: 1.0000e-06\n",
      "Epoch 146/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.7837\n",
      "Epoch 146: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7837 - val_loss: 0.0697 - val_accuracy: 0.8024 - lr: 1.0000e-06\n",
      "Epoch 147/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.7840\n",
      "Epoch 147: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0678 - accuracy: 0.7840 - val_loss: 0.0697 - val_accuracy: 0.8043 - lr: 1.0000e-06\n",
      "Epoch 148/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.7861\n",
      "Epoch 148: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7858 - val_loss: 0.0697 - val_accuracy: 0.8041 - lr: 1.0000e-06\n",
      "Epoch 149/500\n",
      "560/572 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.7845\n",
      "Epoch 149: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "\n",
      "Epoch 149: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0678 - accuracy: 0.7845 - val_loss: 0.0697 - val_accuracy: 0.8051 - lr: 1.0000e-06\n",
      "Epoch 150/500\n",
      "563/572 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.7844\n",
      "Epoch 150: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7841 - val_loss: 0.0697 - val_accuracy: 0.8046 - lr: 1.0000e-07\n",
      "Epoch 151/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.7841\n",
      "Epoch 151: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0678 - accuracy: 0.7839 - val_loss: 0.0697 - val_accuracy: 0.8048 - lr: 1.0000e-07\n",
      "Epoch 152/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.7819\n",
      "Epoch 152: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7821 - val_loss: 0.0697 - val_accuracy: 0.8051 - lr: 1.0000e-07\n",
      "Epoch 153/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.7830\n",
      "Epoch 153: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7833 - val_loss: 0.0697 - val_accuracy: 0.8053 - lr: 1.0000e-07\n",
      "Epoch 154/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.7841\n",
      "Epoch 154: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "\n",
      "Epoch 154: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7841 - val_loss: 0.0697 - val_accuracy: 0.8046 - lr: 1.0000e-07\n",
      "Epoch 155/500\n",
      "558/572 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.7855\n",
      "Epoch 155: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0678 - accuracy: 0.7856 - val_loss: 0.0697 - val_accuracy: 0.8046 - lr: 1.0000e-08\n",
      "Epoch 156/500\n",
      "565/572 [============================>.] - ETA: 0s - loss: 0.0679 - accuracy: 0.7841\n",
      "Epoch 156: val_accuracy did not improve from 0.80999\n",
      "572/572 [==============================] - 1s 2ms/step - loss: 0.0679 - accuracy: 0.7842 - val_loss: 0.0697 - val_accuracy: 0.8046 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xTrain_for_model_charge, yTrain, test_size=0.1)  # 0.2\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    callbacks=get_callbacks()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.8041\n",
      "Evaluate train acc:  0.8040856719017029\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Evaluate train acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@count_elapsed_time\n",
    "def predecir(d):\n",
    "    y_final = model.predict(d)\n",
    "    y_final = np.round(y_final, 0).astype(int)\n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo transcurrido (en segundos): 0.9034075736999512\n",
      "Aciertos train 88.36%\n"
     ]
    }
   ],
   "source": [
    "y_final = predecir(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"Aciertos train\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo transcurrido (en segundos): 0.1325826644897461\n",
      "Aciertos validacion 87.72%\n"
     ]
    }
   ],
   "source": [
    "y_final = predecir(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"Aciertos validacion\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo transcurrido (en segundos): 0.0647740364074707\n",
      "Aciertos test 87.54%\n"
     ]
    }
   ],
   "source": [
    "y_final = predecir(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2) Modelo RNA medio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_simple(cantidad_entradas, cantidad_salidas):\n",
    "    model = Sequential(name=\"Redsimple\")  # los nombres van sin espacios\n",
    "    model.add(Dense(8 * 4, activation=\"relu\",  input_shape=(cantidad_entradas,)) ) \n",
    "    model.add(Dense(16*4, activation='relu'))\n",
    "    model.add(Dense(16*8, activation='relu'))\n",
    "    model.add(Dense(32*8, activation='relu'))\n",
    "    model.add(Reshape((32, 8)))\n",
    "\n",
    "    model.add(UpSampling1D(size=3))\n",
    "    model.add(Conv1D(12, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(12, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(12, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(16, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(16, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(8, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Dense(24, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(cantidad_salidas, activation=\"softmax\", name='output_layer'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Redsimple\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 32)                512       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 32, 8)             0         \n",
      "                                                                 \n",
      " up_sampling1d (UpSampling1D  (None, 96, 8)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 96, 12)            300       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 96, 12)            444       \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 96, 12)            444       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 94, 12)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 94, 24)            888       \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 94, 24)            1752      \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 94, 32)            2336      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 93, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 93, 32)            3104      \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 93, 32)            3104      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 91, 32)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 91, 24)            2328      \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 91, 24)            1752      \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 91, 16)            1168      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 89, 16)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 89, 16)            784       \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 89, 8)             392       \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 88, 8)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 88, 24)            216       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 88, 16)            400       \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 87, 16)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 87, 16)            0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 87, 128)           2176      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 87, 64)            8256      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 87, 32)            2080      \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 11)                363       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 76,255\n",
      "Trainable params: 76,255\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cantidad_entradas = 15\n",
    "cantidad_salidas = 11\n",
    "\n",
    "model = modelo_simple(cantidad_entradas, cantidad_salidas)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=get_optimizador(),\n",
    "              loss='mean_squared_error',  # categorical_crossentropy sparse_categorical_crossentropy mean_squared_error\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_experimental.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xTrain_for_model_charge, yTrain, test_size=0.1)  # 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.6240\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66478, saving model to models_backup/model-0.6648.h5\n",
      "572/572 [==============================] - 11s 11ms/step - loss: 0.1463 - accuracy: 0.6240 - val_loss: 0.1427 - val_accuracy: 0.6648 - lr: 1.0000e-05\n",
      "Epoch 2/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.1188 - accuracy: 0.6650\n",
      "Epoch 2: val_accuracy did not improve from 0.66478\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.1188 - accuracy: 0.6649 - val_loss: 0.1139 - val_accuracy: 0.6648 - lr: 1.0000e-05\n",
      "Epoch 3/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.6649\n",
      "Epoch 3: val_accuracy did not improve from 0.66478\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.1112 - accuracy: 0.6649 - val_loss: 0.1109 - val_accuracy: 0.6648 - lr: 1.0000e-05\n",
      "Epoch 4/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.6652\n",
      "Epoch 4: val_accuracy did not improve from 0.66478\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.1069 - accuracy: 0.6649 - val_loss: 0.1061 - val_accuracy: 0.6648 - lr: 1.0000e-05\n",
      "Epoch 5/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.1005 - accuracy: 0.6650\n",
      "Epoch 5: val_accuracy did not improve from 0.66478\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.1005 - accuracy: 0.6649 - val_loss: 0.0999 - val_accuracy: 0.6648 - lr: 1.0000e-05\n",
      "Epoch 6/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.6788\n",
      "Epoch 6: val_accuracy improved from 0.66478 to 0.76889, saving model to models_backup/model-0.7689.h5\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0946 - accuracy: 0.6798 - val_loss: 0.0969 - val_accuracy: 0.7689 - lr: 1.0000e-05\n",
      "Epoch 7/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.7671\n",
      "Epoch 7: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0913 - accuracy: 0.7673 - val_loss: 0.0945 - val_accuracy: 0.7610 - lr: 1.0000e-05\n",
      "Epoch 8/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.7730\n",
      "Epoch 8: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0895 - accuracy: 0.7730 - val_loss: 0.0941 - val_accuracy: 0.7519 - lr: 1.0000e-05\n",
      "Epoch 9/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.7723\n",
      "Epoch 9: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0882 - accuracy: 0.7721 - val_loss: 0.0967 - val_accuracy: 0.7177 - lr: 1.0000e-05\n",
      "Epoch 10/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0873 - accuracy: 0.7718\n",
      "Epoch 10: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0873 - accuracy: 0.7718 - val_loss: 0.0984 - val_accuracy: 0.6909 - lr: 1.0000e-05\n",
      "Epoch 11/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.7743\n",
      "Epoch 11: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0865 - accuracy: 0.7744 - val_loss: 0.0989 - val_accuracy: 0.6653 - lr: 1.0000e-05\n",
      "Epoch 12/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0859 - accuracy: 0.7775\n",
      "Epoch 12: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0858 - accuracy: 0.7773 - val_loss: 0.0934 - val_accuracy: 0.7066 - lr: 1.0000e-05\n",
      "Epoch 13/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.7812\n",
      "Epoch 13: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0852 - accuracy: 0.7811 - val_loss: 0.0958 - val_accuracy: 0.6823 - lr: 1.0000e-05\n",
      "Epoch 14/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.7837\n",
      "Epoch 14: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0847 - accuracy: 0.7839 - val_loss: 0.0890 - val_accuracy: 0.7285 - lr: 1.0000e-05\n",
      "Epoch 15/500\n",
      "570/572 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.7868\n",
      "Epoch 15: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0842 - accuracy: 0.7867 - val_loss: 0.0891 - val_accuracy: 0.7221 - lr: 1.0000e-05\n",
      "Epoch 16/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.7887\n",
      "Epoch 16: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 6s 10ms/step - loss: 0.0838 - accuracy: 0.7887 - val_loss: 0.0874 - val_accuracy: 0.7320 - lr: 1.0000e-05\n",
      "Epoch 17/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.7907\n",
      "Epoch 17: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0833 - accuracy: 0.7907 - val_loss: 0.0873 - val_accuracy: 0.7275 - lr: 1.0000e-05\n",
      "Epoch 18/500\n",
      "566/572 [============================>.] - ETA: 0s - loss: 0.0830 - accuracy: 0.7922\n",
      "Epoch 18: val_accuracy did not improve from 0.76889\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0829 - accuracy: 0.7922 - val_loss: 0.0848 - val_accuracy: 0.7600 - lr: 1.0000e-05\n",
      "Epoch 19/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.7926\n",
      "Epoch 19: val_accuracy improved from 0.76889 to 0.77307, saving model to models_backup/model-0.7731.h5\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0826 - accuracy: 0.7925 - val_loss: 0.0840 - val_accuracy: 0.7731 - lr: 1.0000e-05\n",
      "Epoch 20/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.7930\n",
      "Epoch 20: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0823 - accuracy: 0.7930 - val_loss: 0.0853 - val_accuracy: 0.7366 - lr: 1.0000e-05\n",
      "Epoch 21/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0820 - accuracy: 0.7947\n",
      "Epoch 21: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0820 - accuracy: 0.7947 - val_loss: 0.0850 - val_accuracy: 0.7374 - lr: 1.0000e-05\n",
      "Epoch 22/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.7946\n",
      "Epoch 22: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0817 - accuracy: 0.7946 - val_loss: 0.0867 - val_accuracy: 0.7088 - lr: 1.0000e-05\n",
      "Epoch 23/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.7956\n",
      "Epoch 23: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0813 - accuracy: 0.7953 - val_loss: 0.0889 - val_accuracy: 0.6739 - lr: 1.0000e-05\n",
      "Epoch 24/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.7973\n",
      "Epoch 24: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0811 - accuracy: 0.7971 - val_loss: 0.0849 - val_accuracy: 0.7241 - lr: 1.0000e-05\n",
      "Epoch 25/500\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.7980\n",
      "Epoch 25: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0808 - accuracy: 0.7980 - val_loss: 0.0851 - val_accuracy: 0.7157 - lr: 1.0000e-05\n",
      "Epoch 26/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.0806 - accuracy: 0.7981\n",
      "Epoch 26: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0806 - accuracy: 0.7980 - val_loss: 0.0841 - val_accuracy: 0.7354 - lr: 1.0000e-05\n",
      "Epoch 27/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.7980\n",
      "Epoch 27: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0804 - accuracy: 0.7981 - val_loss: 0.0828 - val_accuracy: 0.7492 - lr: 1.0000e-05\n",
      "Epoch 28/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567/572 [============================>.] - ETA: 0s - loss: 0.0801 - accuracy: 0.7994\n",
      "Epoch 28: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0802 - accuracy: 0.7992 - val_loss: 0.0841 - val_accuracy: 0.7069 - lr: 1.0000e-05\n",
      "Epoch 29/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0800 - accuracy: 0.7995\n",
      "Epoch 29: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0800 - accuracy: 0.7995 - val_loss: 0.0833 - val_accuracy: 0.7074 - lr: 1.0000e-05\n",
      "Epoch 30/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.7905\n",
      "Epoch 30: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0797 - accuracy: 0.7904 - val_loss: 0.0835 - val_accuracy: 0.5661 - lr: 1.0000e-05\n",
      "Epoch 31/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0796 - accuracy: 0.7642\n",
      "Epoch 31: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0796 - accuracy: 0.7643 - val_loss: 0.0819 - val_accuracy: 0.5562 - lr: 1.0000e-05\n",
      "Epoch 32/500\n",
      "567/572 [============================>.] - ETA: 0s - loss: 0.0794 - accuracy: 0.7437\n",
      "Epoch 32: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0794 - accuracy: 0.7436 - val_loss: 0.0817 - val_accuracy: 0.5585 - lr: 1.0000e-05\n",
      "Epoch 33/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.0792 - accuracy: 0.7383\n",
      "Epoch 33: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0792 - accuracy: 0.7384 - val_loss: 0.0816 - val_accuracy: 0.5604 - lr: 1.0000e-05\n",
      "Epoch 34/500\n",
      "568/572 [============================>.] - ETA: 0s - loss: 0.0790 - accuracy: 0.7379\n",
      "Epoch 34: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0790 - accuracy: 0.7379 - val_loss: 0.0819 - val_accuracy: 0.5540 - lr: 1.0000e-05\n",
      "Epoch 35/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.7356\n",
      "Epoch 35: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0789 - accuracy: 0.7356 - val_loss: 0.0812 - val_accuracy: 0.5730 - lr: 1.0000e-05\n",
      "Epoch 36/500\n",
      "569/572 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.7334\n",
      "Epoch 36: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0787 - accuracy: 0.7333 - val_loss: 0.0818 - val_accuracy: 0.5602 - lr: 1.0000e-05\n",
      "Epoch 37/500\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.7313\n",
      "Epoch 37: val_accuracy did not improve from 0.77307\n",
      "572/572 [==============================] - 5s 9ms/step - loss: 0.0786 - accuracy: 0.7313 - val_loss: 0.0809 - val_accuracy: 0.5789 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    callbacks=get_callbacks()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "#print('Evaluate train acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo transcurrido (en segundos): 3.888659715652466\n",
      "Aciertos train 86.72%\n"
     ]
    }
   ],
   "source": [
    "y_final = predecir(X_train)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_train)[2]\n",
    "print(\"Aciertos train\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo transcurrido (en segundos): 0.43511319160461426\n",
      "Aciertos validacion 86.72%\n"
     ]
    }
   ],
   "source": [
    "y_final = predecir(X_test)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, y_test)[2]\n",
    "print(\"Aciertos validacion\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo transcurrido (en segundos): 0.3243682384490967\n",
      "Aciertos test 86.0%\n"
     ]
    }
   ],
   "source": [
    "y_final = predecir(xTest_for_model_charge)\n",
    "aciertos = calcular_porcentajes_aciertos(y_final, yTest)[2]\n",
    "print(\"Aciertos test\", aciertos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia usada"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/06/visualizing-sounds-librosa/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=iCwMQJnKk2c\n",
    "https://github.com/musikalkemist/DeepLearningForAudioWithPython\n",
    "https://towardsdatascience.com/extract-features-of-music-75a3f9bc265d\n",
    "https://programmerclick.com/article/4926701185/\n",
    "\n",
    "https://www.cienciadedatos.net/documentos/py19-pca-python.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
