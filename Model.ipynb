{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.1.1 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler=pickle.load(open('std_scaler.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_tensorflow(data):\n",
    "    instrument_list = data.iloc[:, -11:]\n",
    "    train = data.iloc[:, 1:-11]\n",
    "    X = scaler.transform(np.array(train, dtype=float))\n",
    "    y = instrument_list\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48680, 26) (48680, 11)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('train.csv')\n",
    "X, y = prepare_data_tensorflow(test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, LeakyReLU, Conv1D, Conv2D, Flatten, MaxPooling2D, Input\n",
    "from tensorflow.keras.layers import BatchNormalization, InputLayer, Reshape, Activation, GlobalAveragePooling1D, Normalization\n",
    "from tensorflow.keras.layers import AveragePooling2D, AveragePooling1D, UpSampling1D, UpSampling2D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "def get_optimizador():\n",
    "    adam = Adam(learning_rate=1e-5)\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, RemoteMonitor, TerminateOnNaN, BackupAndRestore\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "def get_callbacks(name=\"model\"):\n",
    "    #EarlyStopping, detener el entrenamiento una vez que su pérdida comienza a aumentar\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='accuracy',\n",
    "        patience=8, #argumento de patience representa el número de épocas antes de detenerse una vez que su pérdida comienza a aumentar (deja de mejorar).\n",
    "        min_delta=0,  #es un umbral para cuantificar una pérdida en alguna época como mejora o no. Si la diferencia de pérdida es inferior a min_delta , se cuantifica como no mejora. Es mejor dejarlo como 0 ya que estamos interesados ​​en cuando la pérdida empeora.\n",
    "        restore_best_weights=True,\n",
    "        mode='max')\n",
    "\n",
    "    #ReduceLROnPlateau, que si el entrenamiento no mejora tras unos epochs específicos, reduce el valor de learning rate del modelo\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='loss', \n",
    "        factor=0.1, \n",
    "        patience=5, \n",
    "        min_delta=1e-4, \n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Saves Keras model after each epoch\n",
    "    #Para algunos casos es importante saber cual entrenamiento fue mejor, \n",
    "    #este callback guarda el modelo tras cada epoca completada con el fin de si luego se desea un registro de pesos para cada epoca\n",
    "    #Se ha usado este callback para poder optener el mejor modelo de pesos, sobretodo en la red neuronal creada desde cero\n",
    "    #siendo de gran utilidad para determinar el como ir modificando los layer hasta obtener el mejor modelo\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath='models_backup/' + name +'-{val_accuracy:.4f}.h5', \n",
    "        monitor='val_accuracy',\n",
    "        verbose=1, \n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False\n",
    "    )\n",
    "\n",
    "    remote_monitor = RemoteMonitor(\n",
    "        root='http://localhost:6006',\n",
    "        path='/publish/epoch/end/',\n",
    "        field='data',\n",
    "        headers=None,\n",
    "        send_as_json=False\n",
    "    )\n",
    "    \n",
    "    backup_restore = BackupAndRestore(backup_dir=\"backup\")\n",
    "    \n",
    "    proteccion_nan_loss = TerminateOnNaN()\n",
    "\n",
    "    \n",
    "    callbacks_list = [early_stop, reduce_lr, checkpointer, proteccion_nan_loss, backup_restore]#, remote_monitor]\n",
    "    \n",
    "    return callbacks_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testear_modelo(model):\n",
    "    print(\"\\n\"*5)\n",
    "    print(\"*\"*20)\n",
    "    test = pd.read_csv('test.csv')\n",
    "    X, y = prepare_data_tensorflow(test)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X, y)\n",
    "    \n",
    "    print('test_acc: ', test_acc)\n",
    "    \n",
    "\n",
    "def re_train(model, name=\"model\"):\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=500,\n",
    "                        #batch_size=64,\n",
    "                        verbose=1,\n",
    "                        callbacks=get_callbacks(name)\n",
    "                        )\n",
    "    return history, model\n",
    "\n",
    "\n",
    "def show_result(history):\n",
    "    print(plot_history(history)) \n",
    "\n",
    "def evaluate(model):\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print('Evaluate train acc: ', test_acc)\n",
    "    \n",
    "\n",
    "def train_evaluate_model(model_train, name=\"model\"):\n",
    "    model_train.compile(optimizer=get_optimizador(),\n",
    "                  loss='mean_squared_error',  # categorical_crossentropy sparse_categorical_crossentropy\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history, model = re_train(model_train, name)\n",
    "\n",
    "    show_result(history)\n",
    "    \n",
    "    print(\"\\n\"*5)\n",
    "    print(\"*\"*20)\n",
    "\n",
    "    evaluate(model_train)\n",
    "    \n",
    "    testear_modelo(model_train)    \n",
    "    \n",
    "\n",
    "    model_train.save(\"models/model_\" + name + \".h5\")\n",
    "    print(\"\\n\"*5)\n",
    "    \n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RedBasica\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                1728      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 11)                187       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 87,019\n",
      "Trainable params: 87,019\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.core.dense.Dense object at 0x7f105c0dbb20> and <keras.layers.core.dropout.Dropout object at 0x7f105c0db400>).\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.layers.core.dense.Dense object at 0x7f105c155d00> and <keras.layers.core.dense.Dense object at 0x7f105c0dbb20>).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (128,) and (64,) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m modelo_basico(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbasico\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain_evaluate_model\u001b[0;34m(model_train, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_evaluate_model\u001b[39m(model_train, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     33\u001b[0m     model_train\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mget_optimizador(),\n\u001b[1;32m     34\u001b[0m                   loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# categorical_crossentropy sparse_categorical_crossentropy\u001b[39;00m\n\u001b[1;32m     35\u001b[0m                   metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m     history, model \u001b[38;5;241m=\u001b[39m \u001b[43mre_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     show_result(history)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mre_train\u001b[0;34m(model, name)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mre_train\u001b[39m(model, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m#batch_size=64,\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history, model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_shape.py:1167\u001b[0m, in \u001b[0;36mTensorShape.assert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"Raises exception if `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \n\u001b[1;32m   1157\u001b[0m \u001b[38;5;124;03mThis method can be used to assert that there exists a shape that both\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;124;03m  ValueError: If `self` and `other` do not represent the same shape.\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_compatible_with(other):\n\u001b[0;32m-> 1167\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapes \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m, other))\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (128,) and (64,) are incompatible"
     ]
    }
   ],
   "source": [
    "def modelo_basico(cantidad_entradas, cantidad_salidas):    \n",
    "    model = Sequential(name=\"RedBasica\")\n",
    "    model.add(Dense(64, activation='relu', input_shape=(cantidad_entradas,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(cantidad_salidas, activation='softmax', name='output_layer'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = modelo_basico(X_train.shape[1], y_train.shape[1])\n",
    "model.summary()\n",
    "\n",
    "model = train_evaluate_model(model, \"basico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    history, model = re_train(model, \"basico_r\")\n",
    "    show_result(history)\n",
    "    evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_simple(cantidad_entradas, cantidad_salidas):\n",
    "    #inputs = Input(shape=(cantidad_entradas,), name=\"Entradas\")\n",
    "    model = Sequential(name=\"Redsimple\")  # los nombres van sin espacios\n",
    "    #model.add(inputs)\n",
    "    model.add(Dense(8 * 4, activation=\"relu\",  input_shape=(cantidad_entradas,)) ) \n",
    "    model.add(Reshape((8, 4)))\n",
    "    #model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "    model.add(UpSampling1D(size=3))\n",
    "    model.add(Conv1D(12, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(12, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(12, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(32, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(24, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(16, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Conv1D(16, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(Conv1D(8, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Dense(24, activation=\"relu\"))\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(MaxPooling1D(pool_size=2, strides=1, padding='valid'))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "\n",
    "    # model.add( AveragePooling1D(pool_size=2, strides=1, padding='valid') )\n",
    "\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(3, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(cantidad_salidas, activation=\"softmax\", name='output_layer'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = modelo_simple(X_train.shape[1], y_train.shape[1])\n",
    "model = train_evaluate_model(model, \"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando el dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_modelos = {\n",
    "    \"82_9\": \"model82_9.h5\",\n",
    "    \"76_6\": \"model76_6.h5\",\n",
    "    \"now\": \"model.h5\"\n",
    "}\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(json_modelos['now'])\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "X, y = prepare_data_tensorflow(test)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X, y)\n",
    "\n",
    "print('test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
